{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116641,"databundleVersionId":14422847,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Imports & Setup\n# Include necessary libraries for data handling, modeling, and feature engineering.\n# Added clustering and PCA for new features.\nimport os\nimport gc\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, Pool\nimport xgboost as xgb\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.160877Z","iopub.execute_input":"2025-11-11T06:54:50.161178Z","iopub.status.idle":"2025-11-11T06:54:50.167446Z","shell.execute_reply.started":"2025-11-11T06:54:50.161153Z","shell.execute_reply":"2025-11-11T06:54:50.166541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Load Data\n# Load from the main path. Use the provided test.csv with 2799 rows.\n# Do not trim predictions; submit for all 2799 rows to match the file.\n# Analysis: Prompt specifies 2799 samples, so use full test without trimming.\nDATA_PATH = \"/kaggle/input/eee-g513\"\n\ntrain_raw = pd.read_csv(f\"{DATA_PATH}/train.csv\")\ntest_raw = pd.read_csv(f\"{DATA_PATH}/test.csv\")\ncolv_path = f\"{DATA_PATH}/cost_of_living.csv\"\ncolv = pd.read_csv(colv_path) if os.path.exists(colv_path) else None\n\nprint(train_raw.shape, test_raw.shape, None if colv is None else colv.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.169045Z","iopub.execute_input":"2025-11-11T06:54:50.169470Z","iopub.status.idle":"2025-11-11T06:54:50.229781Z","shell.execute_reply.started":"2025-11-11T06:54:50.169443Z","shell.execute_reply":"2025-11-11T06:54:50.228904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Merge Cost-of-Living Data\n# Aggregate means for unique keys to handle any duplicates.\n# Keys: Prefer city_id if available, else city+country.\n# Analysis: COL has 1528 rows, potential multiples; aggregating ensures 1:1 merge.\nif colv is not None:\n    if 'city_id' in train_raw.columns and 'city_id' in colv.columns:\n        keys = ['city_id']\n    else:\n        keys = ['city', 'country']\n\n    colv_agg = colv.groupby(keys, as_index=False).mean(numeric_only=True)\n    train = train_raw.merge(colv_agg, on=keys, how='left')\n    test = test_raw.merge(colv_agg, on=keys, how='left')\nelse:\n    train, test = train_raw.copy(), test_raw.copy()\n\ntrain = train.loc[:, ~train.columns.duplicated()]\ntest = test.loc[:, ~test.columns.duplicated()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.230697Z","iopub.execute_input":"2025-11-11T06:54:50.231051Z","iopub.status.idle":"2025-11-11T06:54:50.259784Z","shell.execute_reply.started":"2025-11-11T06:54:50.231029Z","shell.execute_reply":"2025-11-11T06:54:50.258913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Basic Cleaning\n# Define target, drop invalid rows, identify shared features.\n# Analysis: Targets are positive salaries; drop NaN/<=0 to avoid RMSPE issues.\nTARGET = 'salary_average'\ntrain = train[train[TARGET].notna() & (train[TARGET] > 0)].reset_index(drop=True)\n\nfeat_cols = [c for c in train.columns if c in test.columns]\nprint(len(feat_cols), 'shared features')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.260672Z","iopub.execute_input":"2025-11-11T06:54:50.261315Z","iopub.status.idle":"2025-11-11T06:54:50.279566Z","shell.execute_reply.started":"2025-11-11T06:54:50.261281Z","shell.execute_reply":"2025-11-11T06:54:50.278161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Data Types & Numeric Cleanup\n# Convert objects to numeric if possible, winsorize outliers, log-transform skewed positives, impute medians.\n# Analysis: Salaries and COL indicators are skewed; log helps normality. Winsorize prevents extreme influence.\nfor c in feat_cols:\n    if train[c].dtype == 'object':\n        tr_num = pd.to_numeric(train[c], errors='coerce')\n        te_num = pd.to_numeric(test[c], errors='coerce')\n        if tr_num.notna().mean() > 0.95 and te_num.notna().mean() > 0.95:\n            train[c] = tr_num\n            test[c] = te_num\n\nnum_cols = [c for c in feat_cols if pd.api.types.is_numeric_dtype(train[c])]\ncat_cols = [c for c in feat_cols if c not in num_cols]\n\nfor c in num_cols:\n    q1, q99 = train[c].quantile([0.01, 0.99])\n    train[c] = train[c].clip(q1, q99)\n    test[c] = test[c].clip(q1, q99)\n\nfor c in num_cols:\n    if (train[c] > 0).all() and train[c].skew() > 1.0:\n        train[c] = np.log1p(train[c])\n        test[c] = np.log1p(test[c])\n\nfor c in num_cols:\n    med = train[c].median()\n    train[c] = train[c].fillna(med)\n    test[c] = test[c].fillna(med)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.281504Z","iopub.execute_input":"2025-11-11T06:54:50.281808Z","iopub.status.idle":"2025-11-11T06:54:50.503194Z","shell.execute_reply.started":"2025-11-11T06:54:50.281788Z","shell.execute_reply":"2025-11-11T06:54:50.502246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Add Ratio Features\n# Create meaningful ratios from COL indicators to capture relative costs.\n# Analysis: Ratios normalize costs against purchasing power, helping generalize to unseen cities.\ndef add_ratios(df):\n    cand = df.columns.str.lower()\n    def get(name):\n        idx = np.where(cand.str.contains(name))[0]\n        return df.iloc[:, idx[0]] if len(idx) else None\n\n    ppp = get('purchasing')\n    rent = get('rent')\n    groc = get('grocer') or get('grocery')\n    trans = get('transport')\n    rest = get('restaurant')\n    health = get('health')\n    util = get('utilit')\n\n    if ppp is not None:\n        for other in [rent, groc, trans, rest, health, util]:\n            if other is not None:\n                df[f'f_{other.name.split(\"_\")[0]}_over_ppp'] = other / (ppp + 1e-6)\n\n    if rent is not None and groc is not None:\n        df['f_rent_over_groc'] = rent / (groc + 1e-6)\n    if rent is not None and trans is not None:\n        df['f_rent_over_trans'] = rent / (trans + 1e-6)\n\n    return df\n\ntrain = add_ratios(train)\ntest = add_ratios(test)\n\nfeat_cols = [c for c in train.columns if c in test.columns]\nnum_cols = [c for c in feat_cols if pd.api.types.is_numeric_dtype(train[c])]\ncat_cols = [c for c in feat_cols if c not in num_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.504044Z","iopub.execute_input":"2025-11-11T06:54:50.504314Z","iopub.status.idle":"2025-11-11T06:54:50.516596Z","shell.execute_reply.started":"2025-11-11T06:54:50.504287Z","shell.execute_reply":"2025-11-11T06:54:50.515523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------------\n# Cell 7 – Add PCA & K-Means clusters (FIXED)\n# --------------------------------------------------------------\nif len(num_cols) > 10:                     # enough numeric COL columns\n    # ---- robust-scale the numeric block ---------------------------------\n    scaler_pca = RobustScaler()\n    X_num_tr = scaler_pca.fit_transform(train[num_cols])\n    X_num_te = scaler_pca.transform(test[num_cols])\n\n    # ---- PCA (10 components) -------------------------------------------\n    pca = PCA(n_components=10, random_state=42)\n    pca_tr = pca.fit_transform(X_num_tr)\n    pca_te = pca.transform(X_num_te)\n\n    for i in range(pca_tr.shape[1]):\n        train[f'pca_{i}'] = pca_tr[:, i]\n        test[f'pca_{i}']  = pca_te[:, i]\n\n    # ---- K-Means (20 city clusters) ------------------------------------\n    kmeans = KMeans(n_clusters=20, random_state=42, n_init=10)\n    clusters_tr = kmeans.fit_predict(X_num_tr)      # <-- numpy array\n    clusters_te = kmeans.predict(X_num_te)          # <-- numpy array\n\n    # ---- FIX: convert the *array* to a pandas Series first ----------\n    train['city_cluster'] = pd.Series(clusters_tr, index=train.index).astype('category')\n    test['city_cluster']  = pd.Series(clusters_te, index=test.index).astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.517849Z","iopub.execute_input":"2025-11-11T06:54:50.518204Z","iopub.status.idle":"2025-11-11T06:54:50.903988Z","shell.execute_reply.started":"2025-11-11T06:54:50.518177Z","shell.execute_reply":"2025-11-11T06:54:50.903350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Smoothed Target Encoding Function\n# Use higher prior for smoothing to handle sparse categories and unseen data.\n# Analysis: Increased prior (20) smooths more, reducing overfit on rare country/role combos.\ndef kfold_target_encode(df_tr, y_tr, df_te, col, n_splits=5, prior=20.0, min_count=10, noise=0.01):\n    gkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    out = pd.Series(index=df_te.index, dtype='float64')\n    global_mean = y_tr.mean()\n\n    full_key = df_tr[col].astype(str)\n    agg = pd.DataFrame({'key': full_key, 'y': y_tr}).groupby('key')['y'].agg(['sum', 'count'])\n    enc_full = (agg['sum'] + prior * global_mean) / (agg['count'] + prior)\n\n    out.loc[:] = df_te[col].astype(str).map(enc_full).fillna(global_mean).values\n    if noise > 0:\n        out += noise * np.random.randn(len(out))\n\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.904566Z","iopub.execute_input":"2025-11-11T06:54:50.904771Z","iopub.status.idle":"2025-11-11T06:54:50.912176Z","shell.execute_reply.started":"2025-11-11T06:54:50.904754Z","shell.execute_reply":"2025-11-11T06:54:50.911285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Add Target Encodings\n# Encode more levels: country, state, role, interactions.\n# Analysis: State adds granularity where available; clusters from Cell 7 also encoded.\ndef add_te_block(X_tr, y_tr, X_tgt):\n    Xt = X_tgt.copy()\n    gmean = y_tr.mean()\n\n    for col in ['country', 'state', 'role', 'city_cluster']:\n        if col in X_tr.columns and col in Xt.columns:\n            Xt[f'te_{col}'] = kfold_target_encode(X_tr, y_tr, Xt, col)\n\n    for pair in [('country', 'role'), ('country', 'state'), ('state', 'role')]:\n        if set(pair).issubset(X_tr.columns):\n            key_tr = X_tr[pair[0]].astype(str) + '_' + X_tr[pair[1]].astype(str)\n            key_tgt = Xt[pair[0]].astype(str) + '_' + Xt[pair[1]].astype(str)\n            Xt[f'te_{pair[0]}_{pair[1]}'] = kfold_target_encode(pd.DataFrame({'key': key_tr}), y_tr, pd.DataFrame({'key': key_tgt}), 'key')\n\n    return Xt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.914473Z","iopub.execute_input":"2025-11-11T06:54:50.914714Z","iopub.status.idle":"2025-11-11T06:54:50.932872Z","shell.execute_reply.started":"2025-11-11T06:54:50.914695Z","shell.execute_reply":"2025-11-11T06:54:50.931764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: CV Setup & RMSPE Metric\n# GroupKFold by city_id or city to prevent leakage across similar locations.\n# Analysis: Groups ensure unseen cities are properly validated.\ngroup_key = 'city_id' if 'city_id' in train.columns else ('city' if 'city' in train.columns else None)\ngroups = train[group_key] if group_key is not None else pd.Series(['all'] * len(train))\ngkf = GroupKFold(n_splits=5)\n\ndef rmspe(y, yhat, eps=1e-6):\n    y = np.asarray(y, float)\n    yhat = np.asarray(yhat, float)\n    mask = (y > eps) & np.isfinite(y) & np.isfinite(yhat)\n    return np.sqrt(np.mean(((yhat[mask] - y[mask]) / y[mask]) ** 2))\n\ny_true_oof = np.zeros(len(train))\nlgb_oof = np.zeros(len(train))\ncb_oof = np.zeros(len(train))\nxgb_oof = np.zeros(len(train))\nlgb_test = np.zeros(len(test))\ncb_test = np.zeros(len(test))\nxgb_test = np.zeros(len(test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.933798Z","iopub.execute_input":"2025-11-11T06:54:50.934027Z","iopub.status.idle":"2025-11-11T06:54:50.954933Z","shell.execute_reply.started":"2025-11-11T06:54:50.934009Z","shell.execute_reply":"2025-11-11T06:54:50.953871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: LightGBM Model\n# Tune params for deeper trees, more regularization to prevent overfit.\n# Analysis: Lower LR, higher leaves for complexity; early stopping on 500.\nlgb_params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'learning_rate': 0.03,\n    'num_leaves': 63,\n    'min_data_in_leaf': 50,\n    'feature_fraction': 0.7,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'lambda_l1': 2.0,\n    'lambda_l2': 5.0,\n    'max_depth': -1,\n    'verbosity': -1,\n    'seed': 42\n}\n\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].copy()\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].copy()\n\n    X_tr = add_te_block(X_tr, y_tr, X_tr)\n    X_va = add_te_block(train.iloc[tr_idx][feat_cols], y_tr, X_va)\n    X_te = add_te_block(train.iloc[tr_idx][feat_cols], y_tr, test[feat_cols].copy())\n\n    cat_in_use = [c for c in X_tr.columns if not pd.api.types.is_numeric_dtype(X_tr[c])]\n    for c in cat_in_use:\n        X_tr[c] = X_tr[c].astype('category')\n        X_va[c] = X_va[c].astype('category')\n        X_te[c] = X_te[c].astype('category')\n\n    dtr = lgb.Dataset(X_tr, label=np.log1p(y_tr), categorical_feature=cat_in_use)\n    dva = lgb.Dataset(X_va, label=np.log1p(y_va), categorical_feature=cat_in_use)\n\n    model = lgb.train(lgb_params, dtr, valid_sets=[dtr, dva], num_boost_round=8000,\n                      callbacks=[lgb.early_stopping(500, verbose=False)])\n\n    va_pred = np.expm1(model.predict(X_va))\n    te_pred = np.expm1(model.predict(X_te))\n\n    y_true_oof[va_idx] = y_va.values\n    lgb_oof[va_idx] = va_pred\n    lgb_test += te_pred / 5\n\n    print(f'[LGBM] Fold {fold} RMSPE: {rmspe(y_va, va_pred):.4f}')\n\nprint(f'[LGBM] OOF RMSPE: {rmspe(y_true_oof, lgb_oof):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:50.955966Z","iopub.execute_input":"2025-11-11T06:54:50.956265Z","iopub.status.idle":"2025-11-11T06:54:58.730794Z","shell.execute_reply.started":"2025-11-11T06:54:50.956214Z","shell.execute_reply":"2025-11-11T06:54:58.730081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: CatBoost Model\n# Increased iterations, adjusted reg for better generalization.\n# Analysis: CatBoost handles cats natively; longer training with early stop.\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].copy()\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].copy()\n\n    X_tr = add_te_block(X_tr, y_tr, X_tr)\n    X_va = add_te_block(train.iloc[tr_idx][feat_cols], y_tr, X_va)\n    X_te = add_te_block(train.iloc[tr_idx][feat_cols], y_tr, test[feat_cols].copy())\n\n    for df in (X_tr, X_va, X_te):\n        for c in df.columns:\n            if not pd.api.types.is_numeric_dtype(df[c]):\n                df[c] = df[c].astype('object').where(df[c].notna(), 'Unknown').astype(str)\n\n    cat_idx = [X_tr.columns.get_loc(c) for c in X_tr.columns if not pd.api.types.is_numeric_dtype(X_tr[c])]\n\n    tr_pool = Pool(X_tr, np.log1p(y_tr), cat_features=cat_idx)\n    va_pool = Pool(X_va, np.log1p(y_va), cat_features=cat_idx)\n    te_pool = Pool(X_te, cat_features=cat_idx)\n\n    model = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03, depth=9, l2_leaf_reg=8.0,\n                              iterations=15000, random_seed=42, early_stopping_rounds=700, verbose=False)\n    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n\n    va_pred = np.expm1(model.predict(va_pool))\n    te_pred = np.expm1(model.predict(te_pool))\n\n    cb_oof[va_idx] = va_pred\n    cb_test += te_pred / 5\n\n    print(f'[CatBoost] Fold {fold} RMSPE: {rmspe(y_va, va_pred):.4f}')\n\nprint(f'[CatBoost] OOF RMSPE: {rmspe(y_true_oof, cb_oof):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:54:58.731658Z","iopub.execute_input":"2025-11-11T06:54:58.731873Z","iopub.status.idle":"2025-11-11T06:57:43.060285Z","shell.execute_reply.started":"2025-11-11T06:54:58.731855Z","shell.execute_reply":"2025-11-11T06:57:43.059452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: XGBoost Model\n# Adjusted subsample and reg for diversity.\n# Analysis: XGB adds gradient boosting variety; hist method for speed.\nxgb_params = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'learning_rate': 0.03,\n    'max_depth': 9,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'reg_alpha': 1.0,\n    'reg_lambda': 4.0,\n    'n_estimators': 25000,\n    'tree_method': 'hist',\n    'random_state': 42\n}\n\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].copy()\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].copy()\n\n    X_tr = add_te_block(X_tr, y_tr, X_tr)\n    X_va = add_te_block(train.iloc[tr_idx][feat_cols], y_tr, X_va)\n    X_te = add_te_block(train.iloc[tr_idx][feat_cols], y_tr, test[feat_cols].copy())\n\n    for df in (X_tr, X_va, X_te):\n        for c in df.columns:\n            if not pd.api.types.is_numeric_dtype(df[c]):\n                df[c] = pd.Categorical(df[c]).codes.astype('int32')\n\n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(X_tr, np.log1p(y_tr), eval_set=[(X_va, np.log1p(y_va))], verbose=False, early_stopping_rounds=700)\n\n    va_pred = np.expm1(model.predict(X_va))\n    te_pred = np.expm1(model.predict(X_te))\n\n    xgb_oof[va_idx] = va_pred\n    xgb_test += te_pred / 5\n\n    print(f'[XGB] Fold {fold} RMSPE: {rmspe(y_va, va_pred):.4f}')\n\nprint(f'[XGB] OOF RMSPE: {rmspe(y_true_oof, xgb_oof):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:57:43.061458Z","iopub.execute_input":"2025-11-11T06:57:43.061758Z","iopub.status.idle":"2025-11-11T06:59:27.681619Z","shell.execute_reply.started":"2025-11-11T06:57:43.061730Z","shell.execute_reply":"2025-11-11T06:59:27.680890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Stacking Ensemble\n# Meta-learner on OOF predictions; compare to weighted blend.\n# Analysis: Stack captures model correlations; choose best to avoid overfit.\noof_stack = np.vstack([np.log1p(np.clip(lgb_oof, 0, None)),\n                       np.log1p(np.clip(cb_oof, 0, None)),\n                       np.log1p(np.clip(xgb_oof, 0, None))]).T\ny_log = np.log1p(y_true_oof)\n\nscaler = RobustScaler()\nX_meta = scaler.fit_transform(oof_stack)\n\nmeta = Ridge(alpha=0.5, random_state=42, fit_intercept=True)\nmeta.fit(X_meta, y_log)\n\ntest_stack = np.vstack([np.log1p(np.clip(lgb_test, 0, None)),\n                        np.log1p(np.clip(cb_test, 0, None)),\n                        np.log1p(np.clip(xgb_test, 0, None))]).T\ntest_stack = scaler.transform(test_stack)\nstack_pred = np.expm1(meta.predict(test_stack))\n\nstack_oof = np.expm1(meta.predict(X_meta))\nprint(f'OOF RMSPE — Stack: {rmspe(y_true_oof, stack_oof):.5f}')\n\n# Weighted blend search\nbest = (None, 9e9)\nfor a in np.linspace(0.0, 1.0, 11):\n    for b in np.linspace(0.0, 1.0 - a, 11):\n        c = 1.0 - a - b\n        oof = a * lgb_oof + b * cb_oof + c * xgb_oof\n        s = rmspe(y_true_oof, oof)\n        if s < best[1]:\n            best = ((a, b, c), s)\n\nprint(f'Best weights: {best[0]} | RMSPE: {best[1]:.5f}')\n\nw = best[0]\nblend_test = w[0] * lgb_test + w[1] * cb_test + w[2] * xgb_test\n\nuse_stack = rmspe(y_true_oof, stack_oof) <= best[1]\nfinal_test_pred = stack_pred if use_stack else blend_test\nprint(f'Using {\"Stack\" if use_stack else \"Blend\"} | Final OOF RMSPE: {min(rmspe(y_true_oof, stack_oof), best[1]):.5f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:59:27.682264Z","iopub.execute_input":"2025-11-11T06:59:27.682496Z","iopub.status.idle":"2025-11-11T06:59:27.725923Z","shell.execute_reply.started":"2025-11-11T06:59:27.682477Z","shell.execute_reply":"2025-11-11T06:59:27.725019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15: Generate Submission\n# Use full test rows (2799), IDs from test_raw.\n# Analysis: No trimming; match prompt's 2799 samples.\npred = np.asarray(final_test_pred, float)\nid_col = next((c for c in ['ID', 'id', 'Id'] if c in test_raw.columns), None)\nids = test_raw[id_col].values if id_col else np.arange(1, len(test_raw) + 1)\n\nsub = pd.DataFrame({'ID': ids, 'salary_average': pred})\nsub.to_csv('submission.csv', index=False)\nprint('Submission shape:', sub.shape)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:59:27.726878Z","iopub.execute_input":"2025-11-11T06:59:27.727321Z","iopub.status.idle":"2025-11-11T06:59:27.763281Z","shell.execute_reply.started":"2025-11-11T06:59:27.727291Z","shell.execute_reply":"2025-11-11T06:59:27.762563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 16: Cleanup\n# Free memory.\ngc.collect();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:59:27.764083Z","iopub.execute_input":"2025-11-11T06:59:27.764475Z","iopub.status.idle":"2025-11-11T06:59:27.905489Z","shell.execute_reply.started":"2025-11-11T06:59:27.764447Z","shell.execute_reply":"2025-11-11T06:59:27.904550Z"}},"outputs":[],"execution_count":null}]}