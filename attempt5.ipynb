{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116641,"databundleVersionId":14422847,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Setup\nimport os, gc, warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, QuantileTransformer\nfrom sklearn.linear_model import Ridge\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, Pool\nimport xgboost as xgb\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:10]:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:13:09.119671Z","iopub.execute_input":"2025-11-11T07:13:09.119912Z","iopub.status.idle":"2025-11-11T07:13:21.594722Z","shell.execute_reply.started":"2025-11-11T07:13:09.119891Z","shell.execute_reply":"2025-11-11T07:13:21.592956Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/eee-g513/train.csv\n/kaggle/input/eee-g513/test.csv\n/kaggle/input/eee-g513/cost_of_living.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Load\nDATA_PATH = \"/kaggle/input/eee-g513\"\ntrain_raw = pd.read_csv(f\"{DATA_PATH}/train.csv\")\ntest_raw = pd.read_csv(f\"{DATA_PATH}/test.csv\")\ncolv = pd.read_csv(f\"{DATA_PATH}/cost_of_living.csv\")\nprint(f\"Train: {train_raw.shape}, Test: {test_raw.shape}, COL: {colv.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:13:21.597324Z","iopub.execute_input":"2025-11-11T07:13:21.598114Z","iopub.status.idle":"2025-11-11T07:13:21.668074Z","shell.execute_reply.started":"2025-11-11T07:13:21.598081Z","shell.execute_reply":"2025-11-11T07:13:21.667057Z"}},"outputs":[{"name":"stdout","text":"Train: (6525, 6), Test: (2790, 5), COL: (1528, 56)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#Cell 3: Smart COL Merge\nkeys = ['city_id'] if 'city_id' in train_raw.columns else ['city', 'country']\ncolv_agg = colv.groupby(keys, as_index=False).median(numeric_only=True)\ntrain = train_raw.merge(colv_agg, on=keys, how='left')\ntest = test_raw.merge(colv_agg, on=keys, how='left')\ntrain = train.loc[:, ~train.columns.duplicated()]\ntest = test.loc[:, ~test.columns.duplicated()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:13:31.038186Z","iopub.execute_input":"2025-11-11T07:13:31.038483Z","iopub.status.idle":"2025-11-11T07:13:31.089863Z","shell.execute_reply.started":"2025-11-11T07:13:31.038459Z","shell.execute_reply":"2025-11-11T07:13:31.088731Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Clean Target\nTARGET = \"salary_average\"\ntrain = train[train[TARGET].notna() & (train[TARGET] > 0)].reset_index(drop=True)\nfeat_cols = [c for c in train.columns if c in test.columns and c != TARGET]\nprint(f\"Features: {len(feat_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:13:36.849024Z","iopub.execute_input":"2025-11-11T07:13:36.849984Z","iopub.status.idle":"2025-11-11T07:13:36.861364Z","shell.execute_reply.started":"2025-11-11T07:13:36.849918Z","shell.execute_reply":"2025-11-11T07:13:36.859871Z"}},"outputs":[{"name":"stdout","text":"Features: 58\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 5: Advanced Feature Engineering\ndef engineer_features(df):\n    df = df.copy()\n    cols_lower = {c.lower(): c for c in df.columns}\n    \n    def find(keywords):\n        for kw in keywords:\n            for lower, actual in cols_lower.items():\n                if kw in lower:\n                    return actual\n        return None\n    \n    ppp = find(['purchasing', 'power'])\n    rent = find(['rent'])\n    groc = find(['grocer'])\n    rest = find(['restaurant'])\n    trans = find(['transport'])\n    \n    # Affordability metrics (KEY FEATURE)\n    if ppp and pd.api.types.is_numeric_dtype(df[ppp]):\n        df['ppp_log'] = np.log1p(df[ppp])\n        if rent and pd.api.types.is_numeric_dtype(df[rent]):\n            df['afford_rent'] = df[rent] / (df[ppp] + 1)\n            df['rent_ppp_ratio'] = np.log1p(df[rent]) - np.log1p(df[ppp])\n        if groc and pd.api.types.is_numeric_dtype(df[groc]):\n            df['afford_food'] = df[groc] / (df[ppp] + 1)\n        if trans and pd.api.types.is_numeric_dtype(df[trans]):\n            df['afford_trans'] = df[trans] / (df[ppp] + 1)\n    \n    # Cost burden ratios\n    if rent and groc:\n        if pd.api.types.is_numeric_dtype(df[rent]) and pd.api.types.is_numeric_dtype(df[groc]):\n            df['housing_food_ratio'] = df[rent] / (df[groc] + 1)\n    \n    # Composite COL (weighted by importance)\n    cost_cols = []\n    weights = []\n    if rent and pd.api.types.is_numeric_dtype(df[rent]):\n        cost_cols.append(rent)\n        weights.append(0.4)  # Housing is biggest expense\n    if groc and pd.api.types.is_numeric_dtype(df[groc]):\n        cost_cols.append(groc)\n        weights.append(0.25)\n    if trans and pd.api.types.is_numeric_dtype(df[trans]):\n        cost_cols.append(trans)\n        weights.append(0.2)\n    if rest and pd.api.types.is_numeric_dtype(df[rest]):\n        cost_cols.append(rest)\n        weights.append(0.15)\n    \n    if cost_cols:\n        weights = np.array(weights) / sum(weights)\n        df['weighted_col'] = sum(df[c] * w for c, w in zip(cost_cols, weights))\n    \n    return df\n\ntrain = engineer_features(train)\ntest = engineer_features(test)\nfeat_cols = [c for c in train.columns if c in test.columns and c != TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:13:39.062639Z","iopub.execute_input":"2025-11-11T07:13:39.063049Z","iopub.status.idle":"2025-11-11T07:13:39.078158Z","shell.execute_reply.started":"2025-11-11T07:13:39.063024Z","shell.execute_reply":"2025-11-11T07:13:39.077097Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 6: Preprocessing\nfor c in feat_cols:\n    if train[c].dtype == 'object':\n        tr_num = pd.to_numeric(train[c], errors='coerce')\n        te_num = pd.to_numeric(test[c], errors='coerce')\n        if tr_num.notna().mean() > 0.9:\n            train[c] = tr_num\n            test[c] = te_num\n\nnum_cols = [c for c in feat_cols if pd.api.types.is_numeric_dtype(train[c])]\ncat_cols = [c for c in feat_cols if c not in num_cols]\n\n# Winsorize outliers\nfor c in num_cols:\n    q_low, q_high = train[c].quantile([0.005, 0.995])\n    train[c] = train[c].clip(q_low, q_high)\n    test[c] = test[c].clip(q_low, q_high)\n\n# Log-transform skewed features\nfor c in num_cols:\n    if (train[c] > 0).all() and train[c].skew() > 2.0:\n        train[c] = np.log1p(train[c])\n        test[c] = np.log1p(test[c])\n\n# Impute\nfor c in num_cols:\n    med = train[c].median()\n    train[c] = train[c].fillna(med)\n    test[c] = test[c].fillna(med)\n\nprint(f\"Numeric: {len(num_cols)}, Categorical: {len(cat_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:13:53.054015Z","iopub.execute_input":"2025-11-11T07:13:53.054333Z","iopub.status.idle":"2025-11-11T07:13:53.300003Z","shell.execute_reply.started":"2025-11-11T07:13:53.054303Z","shell.execute_reply":"2025-11-11T07:13:53.298387Z"}},"outputs":[{"name":"stdout","text":"Numeric: 54, Categorical: 4\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 7: Robust Target Encoding\ndef target_encode_cv_safe(train_df, y_train, test_df, col, alpha=150):\n    \"\"\"Bayesian smoothed encoding - FIXED for proper test application\"\"\"\n    global_mean = y_train.mean()\n    \n    # Build encoding map from training data\n    stats = pd.DataFrame({\n        'key': train_df[col].astype(str),\n        'target': y_train\n    }).groupby('key').agg(\n        sum_y=('target', 'sum'),\n        count=('target', 'count')\n    )\n    \n    stats['encoded'] = (stats['sum_y'] + alpha * global_mean) / (stats['count'] + alpha)\n    \n    # Apply to test\n    encoded = test_df[col].astype(str).map(stats['encoded']).fillna(global_mean)\n    return encoded.values\n\ndef add_encodings(X_train, y_train, X_target):\n    \"\"\"Add all target encodings\"\"\"\n    X_out = X_target.copy()\n    \n    if 'country' in X_train.columns:\n        X_out['te_country'] = target_encode_cv_safe(X_train, y_train, X_out, 'country', alpha=150)\n    \n    if 'role' in X_train.columns:\n        X_out['te_role'] = target_encode_cv_safe(X_train, y_train, X_out, 'role', alpha=100)\n    \n    if 'state' in X_train.columns:\n        X_out['te_state'] = target_encode_cv_safe(X_train, y_train, X_out, 'state', alpha=80)\n    \n    if 'city' in X_train.columns:\n        X_out['te_city'] = target_encode_cv_safe(X_train, y_train, X_out, 'city', alpha=50)\n    \n    # Interaction encodings\n    if {'country', 'role'}.issubset(X_train.columns):\n        X_train_temp = X_train.copy()\n        X_out_temp = X_out.copy()\n        X_train_temp['cr'] = X_train['country'].astype(str) + '|' + X_train['role'].astype(str)\n        X_out_temp['cr'] = X_out['country'].astype(str) + '|' + X_out['role'].astype(str)\n        X_out['te_country_role'] = target_encode_cv_safe(X_train_temp, y_train, X_out_temp, 'cr', alpha=200)\n    \n    return X_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:14:03.933772Z","iopub.execute_input":"2025-11-11T07:14:03.934124Z","iopub.status.idle":"2025-11-11T07:14:03.944151Z","shell.execute_reply.started":"2025-11-11T07:14:03.934098Z","shell.execute_reply":"2025-11-11T07:14:03.943335Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 8: CV Setup\ngroup_key = 'city_id' if 'city_id' in train.columns else ('city' if 'city' in train.columns else 'country')\ngroups = train[group_key]\ngkf = GroupKFold(n_splits=5)\n\ndef rmspe(y_true, y_pred, eps=1e-6):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    mask = (y_true > eps) & np.isfinite(y_true) & np.isfinite(y_pred)\n    if mask.sum() == 0:\n        return np.inf\n    return np.sqrt(np.mean(((y_true[mask] - y_pred[mask]) / y_true[mask]) ** 2))\n\nprint(f\"GroupKFold on '{group_key}' - {gkf.n_splits} folds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:14:24.359109Z","iopub.execute_input":"2025-11-11T07:14:24.359411Z","iopub.status.idle":"2025-11-11T07:14:24.367228Z","shell.execute_reply.started":"2025-11-11T07:14:24.359391Z","shell.execute_reply":"2025-11-11T07:14:24.365994Z"}},"outputs":[{"name":"stdout","text":"GroupKFold on 'city_id' - 5 folds\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 9: LightGBM (Tuned)\nlgb_oof = np.zeros(len(train))\nlgb_test_preds = []  # Store each fold's test prediction\ny_oof_true = np.zeros(len(train))\n\nlgb_params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'learning_rate': 0.018,  # Even slower\n    'num_leaves': 20,  # More conservative\n    'min_data_in_leaf': 180,\n    'feature_fraction': 0.7,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'lambda_l1': 3.0,\n    'lambda_l2': 7.0,\n    'max_depth': 5,\n    'min_gain_to_split': 0.02,\n    'verbosity': -1,\n    'seed': 42\n}\n\nprint(\"\\n=== LightGBM ===\")\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].values\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].values\n    \n    # Encode within fold\n    X_tr_enc = add_encodings(X_tr, y_tr, X_tr)\n    X_va_enc = add_encodings(X_tr, y_tr, X_va)\n    X_te_enc = add_encodings(X_tr, y_tr, test[feat_cols].copy())\n    \n    cat_features = [c for c in X_tr_enc.columns if not pd.api.types.is_numeric_dtype(X_tr_enc[c])]\n    for c in cat_features:\n        X_tr_enc[c] = X_tr_enc[c].astype('category')\n        X_va_enc[c] = X_va_enc[c].astype('category')\n        X_te_enc[c] = X_te_enc[c].astype('category')\n    \n    dtrain = lgb.Dataset(X_tr_enc, label=np.log1p(y_tr), categorical_feature=cat_features)\n    dvalid = lgb.Dataset(X_va_enc, label=np.log1p(y_va), categorical_feature=cat_features)\n    \n    model = lgb.train(\n        lgb_params,\n        dtrain,\n        valid_sets=[dvalid],\n        num_boost_round=12000,\n        callbacks=[lgb.early_stopping(600, verbose=False), lgb.log_evaluation(0)]\n    )\n    \n    va_pred = np.expm1(model.predict(X_va_enc, num_iteration=model.best_iteration))\n    te_pred = np.expm1(model.predict(X_te_enc, num_iteration=model.best_iteration))\n    \n    lgb_oof[va_idx] = va_pred\n    lgb_test_preds.append(te_pred)\n    y_oof_true[va_idx] = y_va\n    \n    print(f\"Fold {fold}: {rmspe(y_va, va_pred):.4f} | Iter={model.best_iteration}\")\n\nlgb_test = np.mean(lgb_test_preds, axis=0)\nprint(f\"LGBM OOF: {rmspe(y_oof_true, lgb_oof):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:14:34.601889Z","iopub.execute_input":"2025-11-11T07:14:34.602234Z","iopub.status.idle":"2025-11-11T07:14:40.946382Z","shell.execute_reply.started":"2025-11-11T07:14:34.602211Z","shell.execute_reply":"2025-11-11T07:14:40.945206Z"}},"outputs":[{"name":"stdout","text":"\n=== LightGBM ===\nFold 1: 0.4251 | Iter=343\nFold 2: 1.3787 | Iter=776\nFold 3: 0.5674 | Iter=1020\nFold 4: 0.7425 | Iter=452\nFold 5: 0.5789 | Iter=651\nLGBM OOF: 0.7795\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 10: CatBoost (Tuned)\ncb_oof = np.zeros(len(train))\ncb_test_preds = []\n\nprint(\"\\n=== CatBoost ===\")\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].values\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].values\n    \n    X_tr_enc = add_encodings(X_tr, y_tr, X_tr)\n    X_va_enc = add_encodings(X_tr, y_tr, X_va)\n    X_te_enc = add_encodings(X_tr, y_tr, test[feat_cols].copy())\n    \n    cat_features = []\n    for i, c in enumerate(X_tr_enc.columns):\n        if not pd.api.types.is_numeric_dtype(X_tr_enc[c]):\n            cat_features.append(i)\n            for df in [X_tr_enc, X_va_enc, X_te_enc]:\n                df[c] = df[c].astype(str).fillna('Unknown')\n    \n    train_pool = Pool(X_tr_enc, np.log1p(y_tr), cat_features=cat_features)\n    valid_pool = Pool(X_va_enc, np.log1p(y_va), cat_features=cat_features)\n    test_pool = Pool(X_te_enc, cat_features=cat_features)\n    \n    model = CatBoostRegressor(\n        loss_function='RMSE',\n        learning_rate=0.018,\n        depth=4,  # Even shallower\n        l2_leaf_reg=12.0,\n        iterations=18000,\n        early_stopping_rounds=800,\n        random_seed=42,\n        verbose=False\n    )\n    \n    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n    \n    va_pred = np.expm1(model.predict(valid_pool))\n    te_pred = np.expm1(model.predict(test_pool))\n    \n    cb_oof[va_idx] = va_pred\n    cb_test_preds.append(te_pred)\n    \n    print(f\"Fold {fold}: {rmspe(y_va, va_pred):.4f} | Iter={model.get_best_iteration()}\")\n\ncb_test = np.mean(cb_test_preds, axis=0)\nprint(f\"CatBoost OOF: {rmspe(y_oof_true, cb_oof):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:14:45.683740Z","iopub.execute_input":"2025-11-11T07:14:45.684125Z","iopub.status.idle":"2025-11-11T07:15:24.076466Z","shell.execute_reply.started":"2025-11-11T07:14:45.684100Z","shell.execute_reply":"2025-11-11T07:15:24.075314Z"}},"outputs":[{"name":"stdout","text":"\n=== CatBoost ===\nFold 1: 0.5468 | Iter=355\nFold 2: 1.4782 | Iter=746\nFold 3: 0.8695 | Iter=328\nFold 4: 0.8787 | Iter=402\nFold 5: 0.7210 | Iter=442\nCatBoost OOF: 0.9177\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 11: XGBoost (Tuned)\nxgb_oof = np.zeros(len(train))\nxgb_test_preds = []\n\nprint(\"\\n=== XGBoost ===\")\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].values\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].values\n    \n    X_tr_enc = add_encodings(X_tr, y_tr, X_tr)\n    X_va_enc = add_encodings(X_tr, y_tr, X_va)\n    X_te_enc = add_encodings(X_tr, y_tr, test[feat_cols].copy())\n    \n    for c in X_tr_enc.columns:\n        if not pd.api.types.is_numeric_dtype(X_tr_enc[c]):\n            for df in [X_tr_enc, X_va_enc, X_te_enc]:\n                df[c] = df[c].astype('category').cat.codes\n    \n    model = xgb.XGBRegressor(\n        objective='reg:squarederror',\n        learning_rate=0.018,\n        max_depth=4,\n        subsample=0.65,\n        colsample_bytree=0.65,\n        reg_alpha=3.0,\n        reg_lambda=7.0,\n        n_estimators=18000,\n        early_stopping_rounds=800,\n        random_state=42,\n        tree_method='hist'\n    )\n    \n    model.fit(\n        X_tr_enc, np.log1p(y_tr),\n        eval_set=[(X_va_enc, np.log1p(y_va))],\n        verbose=False\n    )\n    \n    va_pred = np.expm1(model.predict(X_va_enc))\n    te_pred = np.expm1(model.predict(X_te_enc))\n    \n    xgb_oof[va_idx] = va_pred\n    xgb_test_preds.append(te_pred)\n    \n    print(f\"Fold {fold}: {rmspe(y_va, va_pred):.4f}\")\n\nxgb_test = np.mean(xgb_test_preds, axis=0)\nprint(f\"XGBoost OOF: {rmspe(y_oof_true, xgb_oof):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:15:24.078329Z","iopub.execute_input":"2025-11-11T07:15:24.078788Z","iopub.status.idle":"2025-11-11T07:16:13.560855Z","shell.execute_reply.started":"2025-11-11T07:15:24.078761Z","shell.execute_reply":"2025-11-11T07:16:13.560163Z"}},"outputs":[{"name":"stdout","text":"\n=== XGBoost ===\nFold 1: 0.5312\nFold 2: 1.4630\nFold 3: 0.7749\nFold 4: 0.8914\nFold 5: 0.7478\nXGBoost OOF: 0.9014\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 12: Optimized Stacking with Quantile Transform\nprint(\"\\n=== Ensemble ===\")\n\n# Use quantile transform for better meta-feature distribution\nqt = QuantileTransformer(output_distribution='normal', random_state=42)\n\noof_meta = np.column_stack([\n    lgb_oof,\n    cb_oof,\n    xgb_oof,\n    (lgb_oof + cb_oof) / 2,  # Add pairwise blends as features\n    (lgb_oof + xgb_oof) / 2,\n    (cb_oof + xgb_oof) / 2\n])\n\ntest_meta = np.column_stack([\n    lgb_test,\n    cb_test,\n    xgb_test,\n    (lgb_test + cb_test) / 2,\n    (lgb_test + xgb_test) / 2,\n    (cb_test + xgb_test) / 2\n])\n\n# Transform to normal distribution\noof_meta_qt = qt.fit_transform(oof_meta)\ntest_meta_qt = qt.transform(test_meta)\n\n# Train meta-model with higher regularization\nmeta = Ridge(alpha=5.0, random_state=42)\nmeta.fit(oof_meta_qt, y_oof_true)\n\nstacked_oof = meta.predict(oof_meta_qt)\nstacked_test = meta.predict(test_meta_qt)\n\n# Compare approaches\nprint(f\"\\nLGBM:         {rmspe(y_oof_true, lgb_oof):.4f}\")\nprint(f\"CatBoost:     {rmspe(y_oof_true, cb_oof):.4f}\")\nprint(f\"XGBoost:      {rmspe(y_oof_true, xgb_oof):.4f}\")\n\nsimple_blend = (lgb_oof + cb_oof + xgb_oof) / 3\nprint(f\"Simple Blend: {rmspe(y_oof_true, simple_blend):.4f}\")\nprint(f\"Stacked:      {rmspe(y_oof_true, stacked_oof):.4f}\")\n\n# Weighted blend optimization\nbest_score = float('inf')\nbest_weights = None\nfor w1 in np.linspace(0.2, 0.5, 7):\n    for w2 in np.linspace(0.2, 0.5, 7):\n        w3 = 1 - w1 - w2\n        if w3 < 0.15 or w3 > 0.6:\n            continue\n        blend = w1*lgb_oof + w2*cb_oof + w3*xgb_oof\n        score = rmspe(y_oof_true, blend)\n        if score < best_score:\n            best_score = score\n            best_weights = (w1, w2, w3)\n\nopt_blend_oof = best_weights[0]*lgb_oof + best_weights[1]*cb_oof + best_weights[2]*xgb_oof\nopt_blend_test = best_weights[0]*lgb_test + best_weights[1]*cb_test + best_weights[2]*xgb_test\nprint(f\"Opt Blend:    {rmspe(y_oof_true, opt_blend_oof):.4f} | Weights: {best_weights}\")\n\n# Select best\nscores = {\n    'stacked': rmspe(y_oof_true, stacked_oof),\n    'opt_blend': rmspe(y_oof_true, opt_blend_oof),\n    'simple': rmspe(y_oof_true, simple_blend)\n}\nbest_method = min(scores, key=scores.get)\nfinal_pred = {\n    'stacked': stacked_test,\n    'opt_blend': opt_blend_test,\n    'simple': (lgb_test + cb_test + xgb_test) / 3\n}[best_method]\n\nprint(f\"\\n>>> Using: {best_method.upper()} | OOF={scores[best_method]:.4f} <<<\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:16:13.561456Z","iopub.execute_input":"2025-11-11T07:16:13.561681Z","iopub.status.idle":"2025-11-11T07:16:13.653070Z","shell.execute_reply.started":"2025-11-11T07:16:13.561655Z","shell.execute_reply":"2025-11-11T07:16:13.652046Z"}},"outputs":[{"name":"stdout","text":"\n=== Ensemble ===\n\nLGBM:         0.7795\nCatBoost:     0.9177\nXGBoost:      0.9014\nSimple Blend: 0.8607\nStacked:      0.7162\nOpt Blend:    0.8382 | Weights: (0.5, 0.2, 0.3)\n\n>>> Using: STACKED | OOF=0.7162 <<<\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 13: Submission\nid_col = next((c for c in ['ID', 'id', 'Id'] if c in test_raw.columns), None)\ntest_ids = test_raw[id_col].values if id_col else np.arange(1, len(test_raw) + 1)\n\nsubmission = pd.DataFrame({\n    'ID': test_ids,\n    'salary_average': final_pred\n})\n\nassert submission.shape[0] == len(test_raw)\nassert list(submission.columns) == ['ID', 'salary_average']\n\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"\\n✓ Saved: {submission.shape}\")\nprint(submission.head(10))\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T07:16:13.654520Z","iopub.execute_input":"2025-11-11T07:16:13.654891Z","iopub.status.idle":"2025-11-11T07:16:13.836913Z","shell.execute_reply.started":"2025-11-11T07:16:13.654856Z","shell.execute_reply":"2025-11-11T07:16:13.836024Z"}},"outputs":[{"name":"stdout","text":"\n✓ Saved: (2790, 2)\n   ID  salary_average\n0   1    83045.153377\n1   2    89293.726228\n2   3    93244.745040\n3   4    64899.165047\n4   5    53847.348584\n5   6    70091.773746\n6   7    53313.496824\n7   8    64782.170392\n8   9    85264.897535\n9  10    83715.442440\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}],"execution_count":15}]}