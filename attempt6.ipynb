{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116641,"databundleVersionId":14422847,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Imports and Setup\nimport os, gc, warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Ridge\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, Pool\nimport xgboost as xgb\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# List input files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:10]:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:23:40.038885Z","iopub.execute_input":"2025-11-14T15:23:40.039140Z","iopub.status.idle":"2025-11-14T15:23:51.395763Z","shell.execute_reply.started":"2025-11-14T15:23:40.039116Z","shell.execute_reply":"2025-11-14T15:23:51.394767Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/eee-g513/train.csv\n/kaggle/input/eee-g513/test.csv\n/kaggle/input/eee-g513/cost_of_living.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Load Data with Proper Handling\nDATA_PATH = \"/kaggle/input/eee-g513\"\n\ntrain_raw = pd.read_csv(f\"{DATA_PATH}/train.csv\")\ntest_raw = pd.read_csv(f\"{DATA_PATH}/test.csv\")\ncolv_path = f\"{DATA_PATH}/cost_of_living.csv\"\ncolv = pd.read_csv(colv_path) if os.path.exists(colv_path) else None\n\nprint(f\"Train: {train_raw.shape}, Test: {test_raw.shape}, COL: {colv.shape if colv is not None else None}\")\nprint(f\"\\nTarget stats:\\n{train_raw['salary_average'].describe()}\")\nprint(f\"\\nMissing in train:\\n{train_raw.isnull().sum()[train_raw.isnull().sum() > 0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:24:15.491730Z","iopub.execute_input":"2025-11-14T15:24:15.492015Z","iopub.status.idle":"2025-11-14T15:24:15.592566Z","shell.execute_reply.started":"2025-11-14T15:24:15.491993Z","shell.execute_reply":"2025-11-14T15:24:15.591565Z"}},"outputs":[{"name":"stdout","text":"Train: (6525, 6), Test: (2790, 5), COL: (1528, 56)\n\nTarget stats:\ncount      6480.000000\nmean      53169.866584\nstd       27592.877914\nmin        3836.556547\n25%       28697.390996\n50%       54673.064063\n75%       72742.000000\nmax      157747.634644\nName: salary_average, dtype: float64\n\nMissing in train:\nsalary_average    45\ndtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Smart COL Merge - More Aggressive Feature Creation\nif colv is not None:\n    # Identify merge keys\n    if 'city_id' in train_raw.columns and 'city_id' in colv.columns:\n        keys = ['city_id']\n    else:\n        keys = ['city', 'country']\n    \n    # Create MULTIPLE aggregations (mean, median, std) for robustness\n    colv_mean = colv.groupby(keys, as_index=False).mean(numeric_only=True)\n    colv_median = colv.groupby(keys, as_index=False).median(numeric_only=True)\n    colv_std = colv.groupby(keys, as_index=False).std(numeric_only=True)\n    \n    # Rename std columns\n    for c in colv_std.columns:\n        if c not in keys:\n            colv_std.rename(columns={c: f'{c}_std'}, inplace=True)\n    \n    # Merge all\n    train = train_raw.merge(colv_mean, on=keys, how='left', suffixes=('', '_drop'))\n    train = train.merge(colv_median, on=keys, how='left', suffixes=('', '_median'))\n    train = train.merge(colv_std, on=keys, how='left')\n    \n    test = test_raw.merge(colv_mean, on=keys, how='left', suffixes=('', '_drop'))\n    test = test.merge(colv_median, on=keys, how='left', suffixes=('', '_median'))\n    test = test.merge(colv_std, on=keys, how='left')\n    \n    # Drop duplicate columns\n    train = train[[c for c in train.columns if not c.endswith('_drop')]]\n    test = test[[c for c in test.columns if not c.endswith('_drop')]]\nelse:\n    train, test = train_raw.copy(), test_raw.copy()\n\nprint(f\"After merge - Train: {train.shape}, Test: {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:24:20.175902Z","iopub.execute_input":"2025-11-14T15:24:20.176214Z","iopub.status.idle":"2025-11-14T15:24:20.307534Z","shell.execute_reply.started":"2025-11-14T15:24:20.176190Z","shell.execute_reply":"2025-11-14T15:24:20.306261Z"}},"outputs":[{"name":"stdout","text":"After merge - Train: (6525, 165), Test: (2790, 164)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Target Cleaning and Feature Selection\nTARGET = \"salary_average\"\n\n# Remove invalid targets\nbefore = len(train)\ntrain = train[train[TARGET].notna() & (train[TARGET] > 0)].reset_index(drop=True)\nprint(f\"Dropped {before - len(train)} invalid targets\")\n\n# Identify common features (exclude target)\nfeat_cols = [c for c in train.columns if c in test.columns and c != TARGET]\nprint(f\"Features: {len(feat_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:24:22.286453Z","iopub.execute_input":"2025-11-14T15:24:22.286814Z","iopub.status.idle":"2025-11-14T15:24:22.323836Z","shell.execute_reply.started":"2025-11-14T15:24:22.286790Z","shell.execute_reply":"2025-11-14T15:24:22.322833Z"}},"outputs":[{"name":"stdout","text":"Dropped 45 invalid targets\nFeatures: 164\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5: Advanced Feature Engineering\ndef create_interaction_features(df):\n    \"\"\"Create domain-informed interaction features\"\"\"\n    df = df.copy()\n    \n    # Find COL columns\n    cols_lower = {c.lower(): c for c in df.columns}\n    \n    def find_col(keywords):\n        for kw in keywords:\n            for lower, actual in cols_lower.items():\n                if kw in lower:\n                    return actual\n        return None\n    \n    # Key COL indicators\n    ppp = find_col(['purchasing', 'power'])\n    rent = find_col(['rent'])\n    groc = find_col(['grocer'])\n    rest = find_col(['restaurant'])\n    trans = find_col(['transport'])\n    local = find_col(['local_purchasing'])\n    \n    # === NEW: More sophisticated ratios ===\n    if ppp and rent:\n        if pd.api.types.is_numeric_dtype(df[ppp]) and pd.api.types.is_numeric_dtype(df[rent]):\n            df['rent_ppp_ratio'] = df[rent] / (df[ppp] + 1)\n            df['ppp_rent_sqrt'] = np.sqrt(df[ppp] * df[rent])\n    \n    if groc and rest:\n        if pd.api.types.is_numeric_dtype(df[groc]) and pd.api.types.is_numeric_dtype(df[rest]):\n            df['dining_out_ratio'] = df[rest] / (df[groc] + 1)\n    \n    if rent and groc:\n        if pd.api.types.is_numeric_dtype(df[rent]) and pd.api.types.is_numeric_dtype(df[groc]):\n            df['housing_food_burden'] = df[rent] + df[groc]\n            df['housing_food_ratio'] = df[rent] / (df[groc] + 1)\n    \n    # === NEW: Composite indices ===\n    cost_cols = []\n    for col_name in [rent, groc, rest, trans]:\n        if col_name and pd.api.types.is_numeric_dtype(df[col_name]):\n            cost_cols.append(col_name)\n    \n    if len(cost_cols) >= 2:\n        df['composite_col_mean'] = df[cost_cols].mean(axis=1)\n        df['composite_col_max'] = df[cost_cols].max(axis=1)\n        df['composite_col_min'] = df[cost_cols].min(axis=1)\n        df['composite_col_range'] = df['composite_col_max'] - df['composite_col_min']\n    \n    # === NEW: PPP-adjusted indices ===\n    if ppp and len(cost_cols) >= 2:\n        if pd.api.types.is_numeric_dtype(df[ppp]):\n            df['affordability_index'] = df['composite_col_mean'] / (df[ppp] + 1)\n    \n    return df\n\ntrain = create_interaction_features(train)\ntest = create_interaction_features(test)\n\n# Update feature list\nfeat_cols = [c for c in train.columns if c in test.columns and c != TARGET]\nprint(f\"Total features after engineering: {len(feat_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:24:41.241507Z","iopub.execute_input":"2025-11-14T15:24:41.241914Z","iopub.status.idle":"2025-11-14T15:24:41.268532Z","shell.execute_reply.started":"2025-11-14T15:24:41.241887Z","shell.execute_reply":"2025-11-14T15:24:41.267362Z"}},"outputs":[{"name":"stdout","text":"Total features after engineering: 164\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 6: Type Detection and Numeric Preprocessing (LESS restrictive)\n# Convert numeric-like objects\nfor c in feat_cols:\n    if train[c].dtype == 'object':\n        tr_num = pd.to_numeric(train[c], errors='coerce')\n        te_num = pd.to_numeric(test[c], errors='coerce')\n        if tr_num.notna().mean() > 0.8:  # More lenient\n            train[c] = tr_num\n            test[c] = te_num\n\n# Separate numeric and categorical\nnum_cols = [c for c in feat_cols if pd.api.types.is_numeric_dtype(train[c])]\ncat_cols = [c for c in feat_cols if c not in num_cols]\n\nprint(f\"Numeric: {len(num_cols)}, Categorical: {len(cat_cols)}\")\n\n# LESS aggressive outlier handling (1% and 99% instead of 0.5% and 99.5%)\nfor c in num_cols:\n    q_low, q_high = train[c].quantile([0.01, 0.99])\n    train[c] = train[c].clip(q_low, q_high)\n    test[c] = test[c].clip(q_low, q_high)\n\n# Only log-transform VERY skewed features (skew > 3)\nfor c in num_cols:\n    if (train[c] > 0).all() and train[c].skew() > 3.0:\n        train[c] = np.log1p(train[c])\n        test[c] = np.log1p(test[c])\n\n# Impute with median\nfor c in num_cols:\n    med = train[c].median()\n    train[c] = train[c].fillna(med)\n    test[c] = test[c].fillna(med)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:24:50.990868Z","iopub.execute_input":"2025-11-14T15:24:50.991180Z","iopub.status.idle":"2025-11-14T15:24:51.717805Z","shell.execute_reply.started":"2025-11-14T15:24:50.991157Z","shell.execute_reply":"2025-11-14T15:24:51.716741Z"}},"outputs":[{"name":"stdout","text":"Numeric: 160, Categorical: 4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 7: Smoothed Target Encoding (CV-Safe)\ndef smoothed_target_encode(train_df, y_train, test_df, col, alpha=50):\n    \"\"\"Bayesian smoothed mean encoding\"\"\"\n    global_mean = y_train.mean()\n    \n    stats = pd.DataFrame({\n        'key': train_df[col].astype(str),\n        'target': y_train\n    }).groupby('key').agg(\n        sum_y=('target', 'sum'),\n        count=('target', 'count')\n    )\n    \n    stats['encoded'] = (stats['sum_y'] + alpha * global_mean) / (stats['count'] + alpha)\n    encoded = test_df[col].astype(str).map(stats['encoded']).fillna(global_mean)\n    \n    return encoded\n\ndef add_target_encodings(X_train, y_train, X_target):\n    \"\"\"Add all target encodings\"\"\"\n    X_out = X_target.copy()\n    \n    if 'country' in X_train.columns:\n        X_out['te_country'] = smoothed_target_encode(X_train, y_train, X_out, 'country', alpha=100)\n    \n    if 'role' in X_train.columns:\n        X_out['te_role'] = smoothed_target_encode(X_train, y_train, X_out, 'role', alpha=50)\n    \n    if 'state' in X_train.columns:\n        X_out['te_state'] = smoothed_target_encode(X_train, y_train, X_out, 'state', alpha=30)\n    \n    if {'country', 'role'}.issubset(X_train.columns):\n        X_train_temp = X_train.copy()\n        X_out_temp = X_out.copy()\n        X_train_temp['country_role'] = X_train['country'].astype(str) + '_' + X_train['role'].astype(str)\n        X_out_temp['country_role'] = X_out['country'].astype(str) + '_' + X_out['role'].astype(str)\n        X_out['te_country_role'] = smoothed_target_encode(X_train_temp, y_train, X_out_temp, 'country_role', alpha=200)\n    \n    return X_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:25:21.820125Z","iopub.execute_input":"2025-11-14T15:25:21.820504Z","iopub.status.idle":"2025-11-14T15:25:21.833239Z","shell.execute_reply.started":"2025-11-14T15:25:21.820476Z","shell.execute_reply":"2025-11-14T15:25:21.831910Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 8: Setup Cross-Validation\ngroup_key = 'city_id' if 'city_id' in train.columns else ('city' if 'city' in train.columns else 'country')\ngroups = train[group_key] if group_key else pd.Series(range(len(train)))\n\ngkf = GroupKFold(n_splits=5)\n\ndef rmspe(y_true, y_pred, eps=1e-6):\n    \"\"\"Root Mean Square Percentage Error\"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    mask = (y_true > eps) & np.isfinite(y_true) & np.isfinite(y_pred)\n    if mask.sum() == 0:\n        return np.inf\n    return np.sqrt(np.mean(((y_true[mask] - y_pred[mask]) / y_true[mask]) ** 2))\n\nprint(f\"CV Strategy: GroupKFold on '{group_key}' with {gkf.get_n_splits()} folds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:25:41.813482Z","iopub.execute_input":"2025-11-14T15:25:41.813796Z","iopub.status.idle":"2025-11-14T15:25:41.823007Z","shell.execute_reply.started":"2025-11-14T15:25:41.813774Z","shell.execute_reply":"2025-11-14T15:25:41.821398Z"}},"outputs":[{"name":"stdout","text":"CV Strategy: GroupKFold on 'city_id' with 5 folds\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 9: LightGBM with LESS Conservative Hyperparameters\nlgb_oof = np.zeros(len(train))\nlgb_test = np.zeros(len(test))\ny_oof_true = np.zeros(len(train))\n\nlgb_params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'learning_rate': 0.03,  # Slightly higher\n    'num_leaves': 31,  # More complex\n    'min_data_in_leaf': 80,  # Less regularization\n    'feature_fraction': 0.8,  # Use more features\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'lambda_l1': 0.5,  # Much less L1\n    'lambda_l2': 1.0,  # Much less L2\n    'max_depth': -1,  # Unlimited\n    'min_gain_to_split': 0.001,  # Allow more splits\n    'verbosity': -1,\n    'seed': 42\n}\n\nprint(\"\\n=== LightGBM Training ===\")\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].values\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].values\n    \n    X_tr_enc = add_target_encodings(X_tr, y_tr, X_tr)\n    X_va_enc = add_target_encodings(X_tr, y_tr, X_va)\n    X_te_enc = add_target_encodings(X_tr, y_tr, test[feat_cols].copy())\n    \n    cat_features = [c for c in X_tr_enc.columns if not pd.api.types.is_numeric_dtype(X_tr_enc[c])]\n    for c in cat_features:\n        X_tr_enc[c] = X_tr_enc[c].astype('category')\n        X_va_enc[c] = X_va_enc[c].astype('category')\n        X_te_enc[c] = X_te_enc[c].astype('category')\n    \n    dtrain = lgb.Dataset(X_tr_enc, label=np.log1p(y_tr), categorical_feature=cat_features)\n    dvalid = lgb.Dataset(X_va_enc, label=np.log1p(y_va), categorical_feature=cat_features)\n    \n    model = lgb.train(\n        lgb_params,\n        dtrain,\n        valid_sets=[dtrain, dvalid],\n        num_boost_round=10000,\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=300, verbose=False),  # Less patience\n            lgb.log_evaluation(period=0)\n        ]\n    )\n    \n    va_pred = np.expm1(model.predict(X_va_enc, num_iteration=model.best_iteration))\n    te_pred = np.expm1(model.predict(X_te_enc, num_iteration=model.best_iteration))\n    \n    lgb_oof[va_idx] = va_pred\n    lgb_test += te_pred / gkf.n_splits\n    y_oof_true[va_idx] = y_va\n    \n    fold_score = rmspe(y_va, va_pred)\n    print(f\"Fold {fold}: RMSPE = {fold_score:.4f} | Best Iteration = {model.best_iteration}\")\n\nlgb_cv_score = rmspe(y_oof_true, lgb_oof)\nprint(f\"\\nLGBM OOF RMSPE: {lgb_cv_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:25:51.438001Z","iopub.execute_input":"2025-11-14T15:25:51.438918Z","iopub.status.idle":"2025-11-14T15:26:04.463470Z","shell.execute_reply.started":"2025-11-14T15:25:51.438888Z","shell.execute_reply":"2025-11-14T15:26:04.462148Z"}},"outputs":[{"name":"stdout","text":"\n=== LightGBM Training ===\nFold 1: RMSPE = 0.2906 | Best Iteration = 302\nFold 2: RMSPE = 0.9011 | Best Iteration = 1690\nFold 3: RMSPE = 0.7019 | Best Iteration = 496\nFold 4: RMSPE = 0.6947 | Best Iteration = 1341\nFold 5: RMSPE = 0.6669 | Best Iteration = 352\n\nLGBM OOF RMSPE: 0.6502\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 10: CatBoost with Less Conservative Settings\ncb_oof = np.zeros(len(train))\ncb_test = np.zeros(len(test))\n\nprint(\"\\n=== CatBoost Training ===\")\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].values\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].values\n    \n    X_tr_enc = add_target_encodings(X_tr, y_tr, X_tr)\n    X_va_enc = add_target_encodings(X_tr, y_tr, X_va)\n    X_te_enc = add_target_encodings(X_tr, y_tr, test[feat_cols].copy())\n    \n    cat_features = []\n    for i, c in enumerate(X_tr_enc.columns):\n        if not pd.api.types.is_numeric_dtype(X_tr_enc[c]):\n            cat_features.append(i)\n            for df in [X_tr_enc, X_va_enc, X_te_enc]:\n                df[c] = df[c].astype(str).fillna('Unknown')\n    \n    train_pool = Pool(X_tr_enc, np.log1p(y_tr), cat_features=cat_features)\n    valid_pool = Pool(X_va_enc, np.log1p(y_va), cat_features=cat_features)\n    test_pool = Pool(X_te_enc, cat_features=cat_features)\n    \n    model = CatBoostRegressor(\n        loss_function='RMSE',\n        learning_rate=0.03,  # Slightly higher\n        depth=6,  # More depth\n        l2_leaf_reg=3.0,  # Less regularization\n        iterations=15000,\n        early_stopping_rounds=500,  # Less patience\n        random_seed=42,\n        verbose=False,\n        bootstrap_type='Bayesian',  # Try Bayesian bootstrap\n        bagging_temperature=0.5  # Add some randomness\n    )\n    \n    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n    \n    va_pred = np.expm1(model.predict(valid_pool))\n    te_pred = np.expm1(model.predict(test_pool))\n    \n    cb_oof[va_idx] = va_pred\n    cb_test += te_pred / gkf.n_splits\n    \n    fold_score = rmspe(y_va, va_pred)\n    print(f\"Fold {fold}: RMSPE = {fold_score:.4f} | Best Iteration = {model.get_best_iteration()}\")\n\ncb_cv_score = rmspe(y_oof_true, cb_oof)\nprint(f\"\\nCatBoost OOF RMSPE: {cb_cv_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:26:08.236940Z","iopub.execute_input":"2025-11-14T15:26:08.237242Z","iopub.status.idle":"2025-11-14T15:27:13.511153Z","shell.execute_reply.started":"2025-11-14T15:26:08.237219Z","shell.execute_reply":"2025-11-14T15:27:13.510112Z"}},"outputs":[{"name":"stdout","text":"\n=== CatBoost Training ===\nFold 1: RMSPE = 0.3332 | Best Iteration = 558\nFold 2: RMSPE = 1.0070 | Best Iteration = 371\nFold 3: RMSPE = 0.7944 | Best Iteration = 317\nFold 4: RMSPE = 1.0165 | Best Iteration = 386\nFold 5: RMSPE = 0.6828 | Best Iteration = 382\n\nCatBoost OOF RMSPE: 0.7701\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 11: XGBoost with Less Regularization\nxgb_oof = np.zeros(len(train))\nxgb_test = np.zeros(len(test))\n\nprint(\"\\n=== XGBoost Training ===\")\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n    X_tr = train.iloc[tr_idx][feat_cols].copy()\n    y_tr = train.iloc[tr_idx][TARGET].values\n    X_va = train.iloc[va_idx][feat_cols].copy()\n    y_va = train.iloc[va_idx][TARGET].values\n    \n    X_tr_enc = add_target_encodings(X_tr, y_tr, X_tr)\n    X_va_enc = add_target_encodings(X_tr, y_tr, X_va)\n    X_te_enc = add_target_encodings(X_tr, y_tr, test[feat_cols].copy())\n    \n    for c in X_tr_enc.columns:\n        if not pd.api.types.is_numeric_dtype(X_tr_enc[c]):\n            for df in [X_tr_enc, X_va_enc, X_te_enc]:\n                df[c] = df[c].astype('category').cat.codes\n    \n    model = xgb.XGBRegressor(\n        objective='reg:squarederror',\n        learning_rate=0.03,  # Slightly higher\n        max_depth=6,  # More depth\n        subsample=0.8,  # Use more data\n        colsample_bytree=0.8,  # Use more features\n        reg_alpha=0.5,  # Much less L1\n        reg_lambda=1.0,  # Much less L2\n        n_estimators=15000,\n        early_stopping_rounds=500,  # Less patience\n        random_state=42,\n        tree_method='hist',\n        gamma=0.01  # Allow more splits\n    )\n    \n    model.fit(\n        X_tr_enc, np.log1p(y_tr),\n        eval_set=[(X_va_enc, np.log1p(y_va))],\n        verbose=False\n    )\n    \n    va_pred = np.expm1(model.predict(X_va_enc))\n    te_pred = np.expm1(model.predict(X_te_enc))\n    \n    xgb_oof[va_idx] = va_pred\n    xgb_test += te_pred / gkf.n_splits\n    \n    fold_score = rmspe(y_va, va_pred)\n    print(f\"Fold {fold}: RMSPE = {fold_score:.4f}\")\n\nxgb_cv_score = rmspe(y_oof_true, xgb_oof)\nprint(f\"\\nXGBoost OOF RMSPE: {xgb_cv_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:27:13.512703Z","iopub.execute_input":"2025-11-14T15:27:13.513047Z","iopub.status.idle":"2025-11-14T15:27:57.118641Z","shell.execute_reply.started":"2025-11-14T15:27:13.513018Z","shell.execute_reply":"2025-11-14T15:27:57.117391Z"}},"outputs":[{"name":"stdout","text":"\n=== XGBoost Training ===\nFold 1: RMSPE = 0.3518\nFold 2: RMSPE = 1.0184\nFold 3: RMSPE = 0.7420\nFold 4: RMSPE = 1.1227\nFold 5: RMSPE = 0.7584\n\nXGBoost OOF RMSPE: 0.8039\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 12: Smart Weighted Ensemble Based on OOF Performance\nprint(\"\\n=== Ensemble Strategy ===\")\n\n# Calculate weights inversely proportional to OOF RMSPE\nscores = np.array([lgb_cv_score, cb_cv_score, xgb_cv_score])\nprint(f\"Individual OOF Scores:\")\nprint(f\"  LGBM:     {lgb_cv_score:.4f}\")\nprint(f\"  CatBoost: {cb_cv_score:.4f}\")\nprint(f\"  XGBoost:  {xgb_cv_score:.4f}\")\n\n# Inverse weighting (lower error = higher weight)\ninv_scores = 1.0 / scores\nweights = inv_scores / inv_scores.sum()\nprint(f\"\\nCalculated Weights:\")\nprint(f\"  LGBM:     {weights[0]:.4f}\")\nprint(f\"  CatBoost: {weights[1]:.4f}\")\nprint(f\"  XGBoost:  {weights[2]:.4f}\")\n\n# Weighted blend\nweighted_oof = (lgb_oof * weights[0] + \n                cb_oof * weights[1] + \n                xgb_oof * weights[2])\nweighted_test = (lgb_test * weights[0] + \n                 cb_test * weights[1] + \n                 xgb_test * weights[2])\n\nweighted_score = rmspe(y_oof_true, weighted_oof)\nprint(f\"\\nWeighted Blend OOF RMSPE: {weighted_score:.4f}\")\n\n# Also try simple average\nsimple_blend_oof = (lgb_oof + cb_oof + xgb_oof) / 3\nsimple_blend_test = (lgb_test + cb_test + xgb_test) / 3\nsimple_score = rmspe(y_oof_true, simple_blend_oof)\nprint(f\"Simple Average OOF RMSPE:  {simple_score:.4f}\")\n\n# Try Ridge stacking\noof_meta = np.column_stack([\n    np.log1p(np.clip(lgb_oof, 0, None)),\n    np.log1p(np.clip(cb_oof, 0, None)),\n    np.log1p(np.clip(xgb_oof, 0, None))\n])\n\ntest_meta = np.column_stack([\n    np.log1p(np.clip(lgb_test, 0, None)),\n    np.log1p(np.clip(cb_test, 0, None)),\n    np.log1p(np.clip(xgb_test, 0, None))\n])\n\nscaler = RobustScaler()\noof_meta_scaled = scaler.fit_transform(oof_meta)\ntest_meta_scaled = scaler.transform(test_meta)\n\nmeta_model = Ridge(alpha=0.5, random_state=42)  # Less regularization\nmeta_model.fit(oof_meta_scaled, np.log1p(y_oof_true))\n\nstacked_oof = np.expm1(meta_model.predict(oof_meta_scaled))\nstacked_test = np.expm1(meta_model.predict(test_meta_scaled))\nstacked_score = rmspe(y_oof_true, stacked_oof)\nprint(f\"Ridge Stack OOF RMSPE:     {stacked_score:.4f}\")\n\n# Choose best approach\nall_scores = {\n    'LGBM': (lgb_cv_score, lgb_test),\n    'CatBoost': (cb_cv_score, cb_test),\n    'XGBoost': (xgb_cv_score, xgb_test),\n    'Weighted': (weighted_score, weighted_test),\n    'Simple': (simple_score, simple_blend_test),\n    'Stacked': (stacked_score, stacked_test)\n}\n\nbest_method = min(all_scores.items(), key=lambda x: x[1][0])\nfinal_pred = best_method[1][1]\nfinal_score = best_method[1][0]\n\nprint(f\"\\n{'='*50}\")\nprint(f\">>> SELECTED: {best_method[0]} (OOF RMSPE: {final_score:.4f}) <<<\")\nprint(f\"{'='*50}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:28:08.850987Z","iopub.execute_input":"2025-11-14T15:28:08.851274Z","iopub.status.idle":"2025-11-14T15:28:08.872881Z","shell.execute_reply.started":"2025-11-14T15:28:08.851255Z","shell.execute_reply":"2025-11-14T15:28:08.871861Z"}},"outputs":[{"name":"stdout","text":"\n=== Ensemble Strategy ===\nIndividual OOF Scores:\n  LGBM:     0.6502\n  CatBoost: 0.7701\n  XGBoost:  0.8039\n\nCalculated Weights:\n  LGBM:     0.3769\n  CatBoost: 0.3182\n  XGBoost:  0.3049\n\nWeighted Blend OOF RMSPE: 0.7294\nSimple Average OOF RMSPE:  0.7357\nRidge Stack OOF RMSPE:     0.4438\n\n==================================================\n>>> SELECTED: Stacked (OOF RMSPE: 0.4438) <<<\n==================================================\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 13: Create Submission\nid_col = next((c for c in ['ID', 'id', 'Id'] if c in test_raw.columns), None)\n\nif id_col:\n    test_ids = test_raw[id_col].values\nelse:\n    test_ids = np.arange(1, len(test_raw) + 1)\n\nsubmission = pd.DataFrame({\n    'ID': test_ids,\n    'salary_average': final_pred\n})\n\n# Validate\nprint(f\"\\nSubmission Validation:\")\nprint(f\"  Shape: {submission.shape}\")\nprint(f\"  Expected rows: {len(test_raw)}\")\nprint(f\"  Columns: {list(submission.columns)}\")\nprint(f\"  Min prediction: ${submission['salary_average'].min():,.2f}\")\nprint(f\"  Max prediction: ${submission['salary_average'].max():,.2f}\")\nprint(f\"  Mean prediction: ${submission['salary_average'].mean():,.2f}\")\nprint(f\"  Median prediction: ${submission['salary_average'].median():,.2f}\")\n\nassert submission.shape[0] == len(test_raw), \"Row count mismatch!\"\nassert list(submission.columns) == ['ID', 'salary_average'], \"Column mismatch!\"\nassert submission['salary_average'].notna().all(), \"NaN predictions found!\"\nassert (submission['salary_average'] > 0).all(), \"Non-positive predictions found!\"\n\n# Save\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\n✓ Submission saved: submission.csv\")\nprint(\"\\nFirst 10 rows:\")\nprint(submission.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:28:55.170865Z","iopub.execute_input":"2025-11-14T15:28:55.171180Z","iopub.status.idle":"2025-11-14T15:28:55.201670Z","shell.execute_reply.started":"2025-11-14T15:28:55.171161Z","shell.execute_reply":"2025-11-14T15:28:55.200573Z"}},"outputs":[{"name":"stdout","text":"\nSubmission Validation:\n  Shape: (2790, 2)\n  Expected rows: 2790\n  Columns: ['ID', 'salary_average']\n  Min prediction: $4,274.73\n  Max prediction: $121,069.87\n  Mean prediction: $53,029.34\n  Median prediction: $49,504.23\n\n✓ Submission saved: submission.csv\n\nFirst 10 rows:\n   ID  salary_average\n0   1    82789.993766\n1   2    85912.984566\n2   3    91776.589220\n3   4    58337.471630\n4   5    50452.390180\n5   6    61018.806824\n6   7    50738.116998\n7   8    59353.407355\n8   9    86528.310295\n9  10    89281.654768\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 14: Cleanup and Summary\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PIPELINE COMPLETE - SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Final Method:     {best_method[0]}\")\nprint(f\"OOF RMSPE:        {final_score:.4f}\")\nprint(f\"Features Used:    {len(feat_cols)}\")\nprint(f\"Training Samples: {len(train)}\")\nprint(f\"Test Samples:     {len(test)}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:28:58.790728Z","iopub.execute_input":"2025-11-14T15:28:58.791061Z","iopub.status.idle":"2025-11-14T15:28:58.937777Z","shell.execute_reply.started":"2025-11-14T15:28:58.791038Z","shell.execute_reply":"2025-11-14T15:28:58.936624Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nPIPELINE COMPLETE - SUMMARY\n============================================================\nFinal Method:     Stacked\nOOF RMSPE:        0.4438\nFeatures Used:    164\nTraining Samples: 6480\nTest Samples:     2790\n============================================================\n","output_type":"stream"}],"execution_count":17}]}