{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16051f64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ML Competition: Global Salary Prediction with Cost of Living\n",
    "V3.1: Optimized ensemble with dynamic weights + advanced techniques\n",
    "- Scipy optimization for ensemble weights\n",
    "- Out-of-fold predictions for proper meta-learning\n",
    "- Additional feature engineering\n",
    "- Improved regularization\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LIGHTGBM = True\n",
    "except:\n",
    "    HAS_LIGHTGBM = False\n",
    "    print(\"Warning: LightGBM not available.\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"Warning: XGBoost not available.\")\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    \"\"\"Root Mean Square Percentage Error\"\"\"\n",
    "    y_true = np.maximum(y_true, 1)\n",
    "    y_pred = np.maximum(y_pred, 1)\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def optimize_weights(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Find optimal weights for ensemble using scipy optimization\n",
    "    predictions: list of arrays, each with predictions from one model\n",
    "    y_true: true target values\n",
    "    \"\"\"\n",
    "    n_models = len(predictions)\n",
    "    \n",
    "    def objective(weights):\n",
    "        # Normalize weights\n",
    "        weights = np.array(weights) / np.sum(weights)\n",
    "        # Weighted average in log space\n",
    "        ensemble = np.average(predictions, axis=0, weights=weights)\n",
    "        # Convert back and calculate error\n",
    "        ensemble_exp = np.expm1(ensemble)\n",
    "        ensemble_exp = np.maximum(ensemble_exp, 0)\n",
    "        y_true_exp = np.expm1(y_true)\n",
    "        return rmspe(y_true_exp, ensemble_exp)\n",
    "    \n",
    "    # Initialize with equal weights\n",
    "    initial_weights = np.ones(n_models) / n_models\n",
    "    \n",
    "    # Constraints: weights must be positive and sum to 1\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    \n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_weights,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'maxiter': 1000}\n",
    "    )\n",
    "    \n",
    "    optimal_weights = result.x / np.sum(result.x)\n",
    "    return optimal_weights, result.fun\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "col_data = pd.read_csv('cost_of_living.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Cost of living shape: {col_data.shape}\")\n",
    "print(f\"Salary range: ${train['salary_average'].min():.0f} - ${train['salary_average'].max():.0f}\")\n",
    "print(f\"Unique cities in train: {train['city'].nunique()}\")\n",
    "print(f\"Unique cities in test: {test['city'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ENHANCED PRE-PROCESSING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRE-PROCESSING & MERGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Smart imputation for cost of living\n",
    "col_cols = [c for c in col_data.columns if c.startswith('col_')]\n",
    "print(f\"Processing {len(col_cols)} cost-of-living features...\")\n",
    "\n",
    "for col in col_cols:\n",
    "    col_data[col] = col_data.groupby('country')[col].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    global_median = col_data[col].median()\n",
    "    col_data[col] = col_data[col].fillna(global_median if not pd.isna(global_median) else 0)\n",
    "\n",
    "# Merge\n",
    "train_merged = train.merge(col_data, on=['country', 'state', 'city'], how='left')\n",
    "test_merged = test.merge(col_data, on=['country', 'state', 'city'], how='left')\n",
    "\n",
    "print(f\"Train after merge: {train_merged.shape}\")\n",
    "print(f\"Test after merge: {test_merged.shape}\")\n",
    "\n",
    "# Label encoders\n",
    "le_dict = {}\n",
    "for col in ['country', 'state', 'city', 'role']:\n",
    "    le = LabelEncoder()\n",
    "    all_vals = pd.concat([\n",
    "        train_merged[col].astype(str), \n",
    "        test_merged[col].astype(str)\n",
    "    ]).unique()\n",
    "    le.fit(all_vals)\n",
    "    le_dict[col] = le\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ADVANCED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_target_encodings(train_df):\n",
    "    \"\"\"Create target encodings with NO city-level encodings\"\"\"\n",
    "    train_df['log_salary'] = np.log1p(train_df['salary_average'])\n",
    "    \n",
    "    # Count features (for confidence weighting)\n",
    "    country_role_count = train_df.groupby(['country', 'role']).size().to_dict()\n",
    "    state_role_count = train_df.groupby(['state', 'role']).size().to_dict()\n",
    "    country_count = train_df.groupby('country').size().to_dict()\n",
    "    state_count = train_df.groupby('state').size().to_dict()\n",
    "    role_count = train_df.groupby('role').size().to_dict()\n",
    "    \n",
    "    # Country-role (most important for unseen cities)\n",
    "    country_role_mean = train_df.groupby(['country', 'role'])['log_salary'].mean().to_dict()\n",
    "    country_role_median = train_df.groupby(['country', 'role'])['log_salary'].median().to_dict()\n",
    "    country_role_std = train_df.groupby(['country', 'role'])['log_salary'].std().to_dict()\n",
    "    \n",
    "    # State-role interactions\n",
    "    state_role_mean = train_df.groupby(['state', 'role'])['log_salary'].mean().to_dict()\n",
    "    state_role_median = train_df.groupby(['state', 'role'])['log_salary'].median().to_dict()\n",
    "    \n",
    "    # Country level\n",
    "    country_mean = train_df.groupby('country')['log_salary'].mean().to_dict()\n",
    "    country_median = train_df.groupby('country')['log_salary'].median().to_dict()\n",
    "    country_std = train_df.groupby('country')['log_salary'].std().to_dict()\n",
    "    country_min = train_df.groupby('country')['log_salary'].min().to_dict()\n",
    "    country_max = train_df.groupby('country')['log_salary'].max().to_dict()\n",
    "    \n",
    "    # State level\n",
    "    state_mean = train_df.groupby('state')['log_salary'].mean().to_dict()\n",
    "    state_median = train_df.groupby('state')['log_salary'].median().to_dict()\n",
    "    state_std = train_df.groupby('state')['log_salary'].std().to_dict()\n",
    "    \n",
    "    # Role level\n",
    "    role_mean = train_df.groupby('role')['log_salary'].mean().to_dict()\n",
    "    role_median = train_df.groupby('role')['log_salary'].median().to_dict()\n",
    "    role_std = train_df.groupby('role')['log_salary'].std().to_dict()\n",
    "    role_min = train_df.groupby('role')['log_salary'].min().to_dict()\n",
    "    role_max = train_df.groupby('role')['log_salary'].max().to_dict()\n",
    "    \n",
    "    # Global stats\n",
    "    global_mean = train_df['log_salary'].mean()\n",
    "    global_median = train_df['log_salary'].median()\n",
    "    global_std = train_df['log_salary'].std()\n",
    "    \n",
    "    train_df.drop('log_salary', axis=1, inplace=True)\n",
    "    \n",
    "    return {\n",
    "        'country_role_mean': country_role_mean,\n",
    "        'country_role_median': country_role_median,\n",
    "        'country_role_std': country_role_std,\n",
    "        'country_role_count': country_role_count,\n",
    "        'state_role_mean': state_role_mean,\n",
    "        'state_role_median': state_role_median,\n",
    "        'state_role_count': state_role_count,\n",
    "        'country_mean': country_mean,\n",
    "        'country_median': country_median,\n",
    "        'country_std': country_std,\n",
    "        'country_min': country_min,\n",
    "        'country_max': country_max,\n",
    "        'country_count': country_count,\n",
    "        'state_mean': state_mean,\n",
    "        'state_median': state_median,\n",
    "        'state_std': state_std,\n",
    "        'state_count': state_count,\n",
    "        'role_mean': role_mean,\n",
    "        'role_median': role_median,\n",
    "        'role_std': role_std,\n",
    "        'role_min': role_min,\n",
    "        'role_max': role_max,\n",
    "        'role_count': role_count,\n",
    "        'global_mean': global_mean,\n",
    "        'global_median': global_median,\n",
    "        'global_std': global_std\n",
    "    }\n",
    "\n",
    "def engineer_features(df, encodings, le_dict):\n",
    "    \"\"\"Enhanced feature engineering focused on generalization\"\"\"\n",
    "    df = df.copy()\n",
    "    e = encodings\n",
    "    col_cols = [c for c in df.columns if c.startswith('col_')]\n",
    "    \n",
    "    # Label encodings\n",
    "    df['country_encoded'] = le_dict['country'].transform(df['country'].astype(str))\n",
    "    df['state_encoded'] = le_dict['state'].transform(df['state'].astype(str))\n",
    "    df['role_encoded'] = le_dict['role'].transform(df['role'].astype(str))\n",
    "    \n",
    "    # === TARGET ENCODINGS (hierarchical fallback) ===\n",
    "    df['country_role_mean'] = df.apply(\n",
    "        lambda x: e['country_role_mean'].get(\n",
    "            (x['country'], x['role']),\n",
    "            e['country_mean'].get(x['country'], e['global_mean'])\n",
    "        ), axis=1)\n",
    "    df['country_role_median'] = df.apply(\n",
    "        lambda x: e['country_role_median'].get(\n",
    "            (x['country'], x['role']),\n",
    "            e['country_median'].get(x['country'], e['global_median'])\n",
    "        ), axis=1)\n",
    "    df['country_role_std'] = df.apply(\n",
    "        lambda x: e['country_role_std'].get(\n",
    "            (x['country'], x['role']),\n",
    "            e['country_std'].get(x['country'], e['global_std'])\n",
    "        ), axis=1)\n",
    "    \n",
    "    # Confidence features (sample size)\n",
    "    df['country_role_count'] = df.apply(\n",
    "        lambda x: e['country_role_count'].get((x['country'], x['role']), 0), axis=1)\n",
    "    df['country_role_confidence'] = np.log1p(df['country_role_count'])\n",
    "    \n",
    "    # State-Role\n",
    "    df['state_role_mean'] = df.apply(\n",
    "        lambda x: e['state_role_mean'].get(\n",
    "            (x['state'], x['role']),\n",
    "            e['country_role_mean'].get((x['country'], x['role']), e['global_mean'])\n",
    "        ), axis=1)\n",
    "    df['state_role_median'] = df.apply(\n",
    "        lambda x: e['state_role_median'].get(\n",
    "            (x['state'], x['role']),\n",
    "            e['country_role_median'].get((x['country'], x['role']), e['global_median'])\n",
    "        ), axis=1)\n",
    "    df['state_role_count'] = df.apply(\n",
    "        lambda x: e['state_role_count'].get((x['state'], x['role']), 0), axis=1)\n",
    "    \n",
    "    # Country stats\n",
    "    df['country_mean'] = df['country'].map(e['country_mean']).fillna(e['global_mean'])\n",
    "    df['country_median'] = df['country'].map(e['country_median']).fillna(e['global_median'])\n",
    "    df['country_std'] = df['country'].map(e['country_std']).fillna(e['global_std'])\n",
    "    df['country_range'] = df['country'].map(e['country_max']).fillna(e['global_mean']) - \\\n",
    "                          df['country'].map(e['country_min']).fillna(e['global_mean'])\n",
    "    df['country_count'] = df['country'].map(e['country_count']).fillna(0)\n",
    "    \n",
    "    # State stats\n",
    "    df['state_mean'] = df['state'].map(e['state_mean']).fillna(df['country_mean'])\n",
    "    df['state_median'] = df['state'].map(e['state_median']).fillna(df['country_median'])\n",
    "    df['state_std'] = df['state'].map(e['state_std']).fillna(df['country_std'])\n",
    "    df['state_count'] = df['state'].map(e['state_count']).fillna(0)\n",
    "    \n",
    "    # Role stats\n",
    "    df['role_mean'] = df['role'].map(e['role_mean']).fillna(e['global_mean'])\n",
    "    df['role_median'] = df['role'].map(e['role_median']).fillna(e['global_median'])\n",
    "    df['role_std'] = df['role'].map(e['role_std']).fillna(e['global_std'])\n",
    "    df['role_range'] = df['role'].map(e['role_max']).fillna(e['global_mean']) - \\\n",
    "                       df['role'].map(e['role_min']).fillna(e['global_mean'])\n",
    "    df['role_count'] = df['role'].map(e['role_count']).fillna(0)\n",
    "    \n",
    "    # === COST OF LIVING FEATURES ===\n",
    "    for col in col_cols:\n",
    "        df[col] = df[col].fillna(df.groupby('country')[col].transform('median'))\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Basic aggregations\n",
    "    df['col_mean'] = df[col_cols].mean(axis=1)\n",
    "    df['col_median'] = df[col_cols].median(axis=1)\n",
    "    df['col_std'] = df[col_cols].std(axis=1).fillna(0)\n",
    "    df['col_max'] = df[col_cols].max(axis=1)\n",
    "    df['col_min'] = df[col_cols].min(axis=1)\n",
    "    df['col_range'] = df['col_max'] - df['col_min']\n",
    "    df['col_q75'] = df[col_cols].quantile(0.75, axis=1)\n",
    "    df['col_q25'] = df[col_cols].quantile(0.25, axis=1)\n",
    "    df['col_iqr'] = df['col_q75'] - df['col_q25']\n",
    "    df['col_cv'] = df['col_std'] / (df['col_mean'] + 1e-6)\n",
    "    df['col_skew'] = df[col_cols].skew(axis=1)\n",
    "    \n",
    "    # Specific COL categories\n",
    "    df['col_housing_proxy'] = df[[c for c in col_cols[:15]]].mean(axis=1)\n",
    "    df['col_services_proxy'] = df[[c for c in col_cols[15:30]]].mean(axis=1)\n",
    "    df['col_goods_proxy'] = df[[c for c in col_cols[30:]]].mean(axis=1)\n",
    "    \n",
    "    # === KEY INTERACTION FEATURES ===\n",
    "    # Salary per COL unit\n",
    "    df['role_mean_per_col'] = df['role_mean'] / (df['col_mean'] + 1e-6)\n",
    "    df['country_mean_per_col'] = df['country_mean'] / (df['col_mean'] + 1e-6)\n",
    "    df['country_role_mean_per_col'] = df['country_role_mean'] / (df['col_mean'] + 1e-6)\n",
    "    df['state_mean_per_col'] = df['state_mean'] / (df['col_mean'] + 1e-6)\n",
    "    df['state_role_mean_per_col'] = df['state_role_mean'] / (df['col_mean'] + 1e-6)\n",
    "    \n",
    "    # COL-adjusted features\n",
    "    df['col_adjusted_country_role'] = df['country_role_mean'] * np.log1p(df['col_mean'])\n",
    "    df['col_adjusted_state_role'] = df['state_role_mean'] * np.log1p(df['col_mean'])\n",
    "    df['col_adjusted_role'] = df['role_mean'] * np.log1p(df['col_mean'])\n",
    "    \n",
    "    # Ratios\n",
    "    df['country_vs_role_ratio'] = df['country_mean'] / (df['role_mean'] + 1e-6)\n",
    "    df['state_vs_country_ratio'] = df['state_mean'] / (df['country_mean'] + 1e-6)\n",
    "    df['country_role_vs_role_ratio'] = df['country_role_mean'] / (df['role_mean'] + 1e-6)\n",
    "    df['state_role_vs_country_role'] = df['state_role_mean'] / (df['country_role_mean'] + 1e-6)\n",
    "    \n",
    "    # Deviation features\n",
    "    df['country_role_vs_country_dev'] = df['country_role_mean'] - df['country_mean']\n",
    "    df['state_vs_country_dev'] = df['state_mean'] - df['country_mean']\n",
    "    df['state_role_vs_country_role_dev'] = df['state_role_mean'] - df['country_role_mean']\n",
    "    \n",
    "    # Purchasing power proxies\n",
    "    df['purchasing_power_role'] = df['role_mean'] - np.log1p(df['col_mean'])\n",
    "    df['purchasing_power_country'] = df['country_mean'] - np.log1p(df['col_mean'])\n",
    "    df['purchasing_power_country_role'] = df['country_role_mean'] - np.log1p(df['col_mean'])\n",
    "    \n",
    "    # Weighted averages (confidence-weighted)\n",
    "    df['weighted_country_role'] = df['country_role_mean'] * df['country_role_confidence']\n",
    "    df['weighted_state_role'] = df['state_role_mean'] * np.log1p(df['state_role_count'])\n",
    "    \n",
    "    # Variance features\n",
    "    df['salary_variance_country'] = df['country_std'] ** 2\n",
    "    df['salary_variance_role'] = df['role_std'] ** 2\n",
    "    \n",
    "    # Clean up\n",
    "    # <<< THIS IS THE FIX >>>\n",
    "    df = df.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PREPARE DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "valid_idx = train_merged['salary_average'].notna()\n",
    "train_clean = train_merged[valid_idx].copy()\n",
    "print(f\"Samples after removing NaN targets: {len(train_clean)}\")\n",
    "\n",
    "y_log = np.log1p(train_clean['salary_average'])\n",
    "\n",
    "# Get feature columns\n",
    "temp_encodings = create_target_encodings(train_clean.copy())\n",
    "temp_features = engineer_features(train_clean.head(), temp_encodings, le_dict)\n",
    "exclude_cols = ['ID', 'country', 'state', 'city', 'role', 'salary_average', 'city_id']\n",
    "feature_cols = [c for c in temp_features.columns if c not in exclude_cols]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CITY-AWARE CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING WITH CITY-AWARE CV + WEIGHT OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_folds = 5\n",
    "gkf = GroupKFold(n_splits=n_folds)\n",
    "groups = train_clean['city']\n",
    "\n",
    "# Model configurations\n",
    "models_config = []\n",
    "\n",
    "if HAS_LIGHTGBM:\n",
    "    models_config.append({\n",
    "        'name': 'LightGBM_v1',\n",
    "        'model': lgb.LGBMRegressor(\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.008,\n",
    "            max_depth=8,\n",
    "            num_leaves=31,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'cats': ['role_encoded', 'country_encoded', 'state_encoded']\n",
    "    })\n",
    "    \n",
    "    models_config.append({\n",
    "        'name': 'LightGBM_v2',\n",
    "        'model': lgb.LGBMRegressor(\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=6,\n",
    "            num_leaves=15,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.6,\n",
    "            reg_alpha=0.5,\n",
    "            reg_lambda=0.5,\n",
    "            random_state=123,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'cats': ['role_encoded', 'country_encoded', 'state_encoded']\n",
    "    })\n",
    "\n",
    "if HAS_XGBOOST:\n",
    "    models_config.append({\n",
    "        'name': 'XGBoost',\n",
    "        'model': xgb.XGBRegressor(\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.008,\n",
    "            max_depth=7,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            gamma=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method='hist',\n",
    "            enable_categorical=True,\n",
    "            early_stopping_rounds=100\n",
    "        ),\n",
    "        'cats': True\n",
    "    })\n",
    "\n",
    "models_config.append({\n",
    "    'name': 'GradientBoosting',\n",
    "    'model': GradientBoostingRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'cats': False\n",
    "})\n",
    "\n",
    "# Create categorical dtypes for XGBoost\n",
    "cat_dtypes = {}\n",
    "for col_name in ['role', 'country', 'state']:\n",
    "    encoded_col = f\"{col_name}_encoded\"\n",
    "    le = le_dict[col_name]\n",
    "    all_cats = le.transform(le.classes_)\n",
    "    cat_dtypes[encoded_col] = CategoricalDtype(\n",
    "        categories=sorted(np.unique(all_cats)), ordered=False)\n",
    "\n",
    "# Store OOF predictions for weight optimization\n",
    "oof_preds_log = [np.zeros(len(train_clean)) for _ in models_config]\n",
    "all_test_preds_log = [[] for _ in models_config]\n",
    "model_scores = []\n",
    "\n",
    "for model_idx, model_config in enumerate(models_config):\n",
    "    model_name = model_config['name']\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(train_clean, y_log, groups)):\n",
    "        train_fold = train_clean.iloc[train_idx]\n",
    "        val_fold = train_clean.iloc[val_idx]\n",
    "        y_train_log = y_log.iloc[train_idx]\n",
    "        y_val_log = y_log.iloc[val_idx]\n",
    "        y_val = np.expm1(y_val_log)\n",
    "        \n",
    "        # Create encodings\n",
    "        encodings = create_target_encodings(train_fold.copy())\n",
    "        \n",
    "        # Engineer features\n",
    "        X_train = engineer_features(train_fold, encodings, le_dict)[feature_cols]\n",
    "        X_val = engineer_features(val_fold, encodings, le_dict)[feature_cols]\n",
    "        X_test = engineer_features(test_merged, encodings, le_dict)[feature_cols]\n",
    "        \n",
    "        # Train model\n",
    "        model = clone(model_config['model'])\n",
    "        \n",
    "        if model_name.startswith('LightGBM'):\n",
    "            model.fit(X_train, y_train_log,\n",
    "                     eval_set=[(X_val, y_val_log)],\n",
    "                     categorical_feature=model_config['cats'],\n",
    "                     callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "        elif model_name == 'XGBoost':\n",
    "            X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "            X_val.columns = X_val.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "            X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "            \n",
    "            cat_features = [c for c in X_train.columns if c.endswith('_encoded')]\n",
    "            for col in cat_features:\n",
    "                if col in cat_dtypes:\n",
    "                    X_train[col] = X_train[col].astype(cat_dtypes[col])\n",
    "                    X_val[col] = X_val[col].astype(cat_dtypes[col])\n",
    "                    X_test[col] = X_test[col].astype(cat_dtypes[col])\n",
    "            \n",
    "            model.fit(X_train, y_train_log,\n",
    "                     eval_set=[(X_val, y_val_log)],\n",
    "                     verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train, y_train_log)\n",
    "        \n",
    "        # OOF predictions (in log space)\n",
    "        val_preds_log = model.predict(X_val)\n",
    "        oof_preds_log[model_idx][val_idx] = val_preds_log\n",
    "        \n",
    "        # Validate\n",
    "        val_preds = np.expm1(val_preds_log)\n",
    "        val_preds = np.maximum(val_preds, 0)\n",
    "        \n",
    "        score = rmspe(y_val, val_preds)\n",
    "        fold_scores.append(score)\n",
    "        print(f\"Fold {fold+1}: RMSPE = {score:.6f} \"\n",
    "              f\"(held out {len(np.unique(val_fold['city']))} cities)\")\n",
    "        \n",
    "        # Test predictions\n",
    "        test_preds_log = model.predict(X_test)\n",
    "        all_test_preds_log[model_idx].append(test_preds_log)\n",
    "    \n",
    "    avg_score = np.mean(fold_scores)\n",
    "    std_score = np.std(fold_scores)\n",
    "    print(f\"\\n{model_name} CV: {avg_score:.6f} (+/- {std_score:.6f})\")\n",
    "    model_scores.append((model_name, avg_score))\n",
    "\n",
    "# Average test predictions across folds for each model\n",
    "test_preds_avg = [np.mean(preds, axis=0) for preds in all_test_preds_log]\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OPTIMIZE ENSEMBLE WEIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZING ENSEMBLE WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Finding optimal weights using OOF predictions...\")\n",
    "optimal_weights, optimal_score = optimize_weights(oof_preds_log, y_log)\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for name, score in sorted(model_scores, key=lambda x: x[1]):\n",
    "    print(f\"  {name}: {score:.6f}\")\n",
    "\n",
    "print(f\"\\nOptimized ensemble weights:\")\n",
    "for i, (model_config, weight) in enumerate(zip(models_config, optimal_weights)):\n",
    "    print(f\"  {model_config['name']}: {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimized ensemble OOF score: {optimal_score:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CREATE FINAL PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING SUBMISSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use optimized weights\n",
    "final_preds_log = np.average(test_preds_avg, axis=0, weights=optimal_weights)\n",
    "final_preds = np.expm1(final_preds_log)\n",
    "final_preds = np.maximum(final_preds, 0)\n",
    "\n",
    "print(f\"Prediction range: ${final_preds.min():.2f} - ${final_preds.max():.2f}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'salary_average': final_preds\n",
    "})\n",
    "submission.to_csv('submission_v3_1.csv', index=False)\n",
    "print(\"\\nâœ“ Submission saved to 'submission_v3_1.csv'\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission.head(20))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
